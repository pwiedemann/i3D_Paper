%-------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------
\subsection*{Precomputed Radiance Transfer and Extensions} 
PRT was first proposed by \cite{sloan2002precomputed} to address global illumination effects on objects for real-time applications. This technique exploits the limitation of static objects by making a single pre-computation step of the transfer function, allowing fast computations at runtime. 

PRT for dynamic or deformable objects would require pre-computing sequences of \textit{transfer functions} to account for every pose, resulting in datasets that expand in proportion to the number of poses; hence, rapidly becoming unwieldy or inefficient. Memory IO operations can be 3 orders of magnitudes more energy demanding than floating point summations or multiplications \cite{ComputingEnergy}. Our aim is to extend traditional PRT to deformable geometries while preserving a rather manageable and limited storage consumption.

One extension of PRT was introduced by Sloan et al. \shortcite{local-deformable-precomputed-radiance-transfer} to enable transfer of local illumination effects, such as bumps and wrinkles, to arbitrary deformations.  Nevertheless, this method cannot account for distant moving self-shadowing effects, such as cast shadows from a limb to the trunk from an articulated figure. Our intention is to enhance PRT to account for such self-shadowing effects within the modeled range of deformable objects.
%Other approaches \cite{Implicit_Visibility, Implicit_Visibility_2} circumvent the pre-computation problem by proposing an alternative algorithm to efficiently compute an approximation of the Visibility Function (implicit in $T$) at near real-time frame rates. ( However, ... )\\
%Data-based approaches, in principle aim to reduce the dimensionality of the problem, and thus the storage consumption, by exploiting the information of the dataset:\\
%A data-based compression scheme of precomputed radiance transfer matrices is presented in \cite{SkinningPRT}. Precomputed transfer matrices of surface samples, deformed by \textit{skinning}, are clustered and compressed, such that de-compression and interpolation can be performed efficiently.\\  
%An appearance model, that approximates PRT lighting, is presented in  \cite{James_Fatahalian}. The model is based on a reduced state space of deformable shapes that allows only very limited kind of poses/shapes. 

Other approaches, rely on exploiting the information of a specific dataset to reduce the dimensionality of the problem and thus the storage consumption. For instance, Feng et al. \shortcite{SkinningPRT} introduce a data-based compression scheme of precomputed radiance transfer matrices. Precomputed transfer matrices of surface samples, deformed by \textit{skinning}, are clustered and compressed, such that de-compression and interpolation can be performed efficiently.

An appearance model, that approximates PRT lighting, is presented by James and Fatahalian  \shortcite{James_Fatahalian}. The model is based on a reduced state space of deformable shapes that allows only very limited vareity of poses/shapes. 

Similarly, Schneider et al. \shortcite{MoMoPRT} suggest a linear self-shadowing model to predict the coefficients of the transfer function from shape parameters of Morphable Models (MoMo) \cite{MoMo}. Their proposed model shows good results while operating within the reduced shape space of MoMo; nevertheless, our aim is to provide a more powerful PRT-model, that endows good approximations for more arbitrary deformations living within a larger and more generic shape space. To that end, we rather propose a non-linear model with well known strong generalization properties, namely a deep Convolutional Neural Network \cite{DL_nature, ImageNet_CNN, CNN_videoClassification}. 

\subsection*{Deep Learning Appearance on Geometry Data} 
Deep Learning (DL) has been used for appearance predictions before, albeit mostly focusing on learning illumination effects from screen-space data. In  Nalbach et al \shortcite{Nalbach2017b}, Chaitanya et al \shortcite{Chaitanya2017} and Thomas and Forbes \shortcite{DBLP} learning is conducted on image data gathered from the shading buffers to predict illumination effects in screen space. However, image-based approaches often suffer from significant information loss, depending on the visibility of the object, and do not leverage the underlying structure of the geometry. These factors make the learning procedure harder requiring large amounts of training data.

A very recent broad approach to apply deep learning to solving light transport given combined geometry, illumination and material  \cite{Herm2018} is available, which solves via unstructured 3D point clouds into a latent representation for resulting image synthesis. Whilst interactive rates are achieved, the ability to vary viewpoint and light environment in a solved PRT representation is much faster and more suitable for real-time interactive 3D graphics applications where latency is important.

Alternatively, we propose directly learning on \textbf{geometric} data. However, learning on surfaces using CNNs is a rather challenging task. Due to the non-Euclidean nature of the domain basic operations such as the convolution are not well-defined, leading current research down different paths on the effort to adapt CNNs to such domains (we refer the readers to Bronstein et al \shortcite{Geometric_deep_learning} and Masci et al \shortcite{DeepGeoCourse} for a more detailed overview).

One approach, is to circumvent this difficulty by representing the surface data as a probability distribution on a  3D grid and apply volumetric CNNs \cite{3d_ShapeNets}. However, this extrinsic representation has many shortcomings when applied to deformable geometries: They are very sensitive to deformations, are computationally expensive and, equally to the screen-space strategies, require abounding training data. Conversely, strategies for intrinsic shape representations propose different adaptations of CNNs to such domains \cite{ShapeNet1, BoscainiMRB16, CNN_on_Torus}.

In our work, we chose a shape representation that, on the one hand, can endow the underlying shape structure, and on the other hand, supports standard 2D convolution operations. We adopt a parametric approach introduced by Gu et al. \shortcite{gu2002geometry}, called \textit{geometry images}, that transforms a discrete surface into a regularly sampled unit square. This approach has been extended by Praun and Hoppe \shortcite{Spherical_Parametrization} to smooth out some critical limitations of the original work and later validated by Sinha et al \shortcite{sinha2016deep} as suitable framework for Deep Learning purposes.

To the extent of our knowledge, our work is the first to tackle the problem of PRT for deformable objects from a Deep Learning perspective and especially on manifold like data. 


